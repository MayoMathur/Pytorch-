{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision \n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and splitting  dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='/home/thor/Srm/Reasearch Papers /Programming/freecodecamp/pytorchjovian/secondclass', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: /home/thor/Srm/Reasearch Papers /Programming/freecodecamp/pytorchjovian/secondclass\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Split: test\n",
       "    Root Location: /home/thor/Srm/Reasearch Papers /Programming/freecodecamp/pytorchjovian/secondclass\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = MNIST(root='/home/thor/Srm/Reasearch Papers /Programming/freecodecamp/pytorchjovian/secondclass', train=False)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=L size=28x28 at 0x7FF123DF3E48>, tensor(5))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is implemeted using supervised learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Displaying image on graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : tensor(5)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[0]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label :', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[15]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('label: ', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = dataset[10]\n",
    "plt.imshow(image, cmap='gray')\n",
    "print('Label: ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting images to tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch cannot work with images , therefore the images are converted to tensors with the following class of torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch dataset can be transformed into varoius ways. Here we use .ToTensor() to convert images to tensor when laded to memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='/home/thor/Srm/Reasearch Papers /Programming/freecodecamp/pytorchjovian/secondclass', \n",
    "               train=True, transform= transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the image format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) tensor(5)\n"
     ]
    }
   ],
   "source": [
    "image_tensor, label = dataset[0]\n",
    "print(image_tensor.shape, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first element represents color channel. Here it is grayscale therefore its 1, others like RGB have 3. The other tow elements represent the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0039, 0.6039, 0.9922, 0.3529, 0.0000],\n",
      "         [0.0000, 0.5451, 0.9922, 0.7451, 0.0078],\n",
      "         [0.0000, 0.0431, 0.7451, 0.9922, 0.2745],\n",
      "         [0.0000, 0.0000, 0.1373, 0.9451, 0.8824],\n",
      "         [0.0000, 0.0000, 0.0000, 0.3176, 0.9412]]])\n",
      "tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "# Checking sample values of the created tensor.\n",
    "#image_tensor # hepls to the see the entire 28 rows Therefore we use indexing to see a slice of the created tensor.\n",
    "# Slicing\n",
    "print(image_tensor[:, 10:15, 10:15]) # first element for grayscale ':' used for one, second row:row, third column:column\n",
    "print(torch.max(image_tensor), torch.min(image_tensor)) # 1. represents white, 0. for black, values between for gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff11f72fb00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the sliced image tensor\n",
    "plt.imshow(image_tensor[0, 10:15, 10:15], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and validation are done together in order to improve the model as training tends to memorize the information which makes it less efficient over new data. So validation is done by changing the learning rate and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Validation\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use random_split because if we split uniformly the training and validation set will have only particular set of numbers and not all the numbers in each set. For instance training will have number from 0 to 6 and validation set will have numbers from 7 to 9. This will make the training incomplete and the validation set will make a wrong validation criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds, val_ds = random_split(dataset, [50000, 10000])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in batches\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 128\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size) # Shuffling not required because it is only used for evaaluation and not modifying weights and biases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using torch built-in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten image tensor by - (28X28) = 784 before being passed int model. The output of the image is taken 10, where each element of the vector represents image label(0 to 9) respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "# Logitic Regression model\n",
    "model = nn.Linear(input_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0269,  0.0275, -0.0073,  ..., -0.0105, -0.0281, -0.0161],\n",
       "        [-0.0335, -0.0332, -0.0312,  ..., -0.0251, -0.0315,  0.0356],\n",
       "        [ 0.0105,  0.0243,  0.0008,  ..., -0.0200, -0.0264, -0.0024],\n",
       "        ...,\n",
       "        [-0.0085,  0.0252, -0.0263,  ...,  0.0131,  0.0197, -0.0338],\n",
       "        [ 0.0319,  0.0230,  0.0309,  ..., -0.0037,  0.0283,  0.0161],\n",
       "        [-0.0343, -0.0262, -0.0205,  ..., -0.0188,  0.0078, -0.0192]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.weight.shape)\n",
    "model.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.0118,  0.0239,  0.0010,  0.0034, -0.0004, -0.0013, -0.0021, -0.0154,\n",
       "        -0.0069, -0.0280], requires_grad=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.bias.shape)\n",
    "model.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 6, 0, 9, 7, 0, 4, 2, 3, 2, 2, 3, 1, 9, 8, 1, 0, 2, 7, 2, 5, 4, 7, 9,\n",
      "        4, 8, 1, 7, 9, 0, 7, 5, 5, 4, 1, 8, 8, 0, 0, 5, 7, 4, 5, 2, 8, 2, 8, 8,\n",
      "        2, 1, 2, 0, 5, 9, 6, 4, 5, 6, 9, 6, 8, 9, 3, 2, 2, 4, 3, 8, 3, 5, 9, 9,\n",
      "        5, 1, 0, 1, 1, 0, 0, 0, 7, 3, 3, 2, 8, 3, 9, 4, 0, 0, 1, 0, 1, 2, 1, 9,\n",
      "        1, 3, 4, 5, 7, 1, 0, 1, 8, 5, 0, 8, 9, 4, 0, 7, 8, 5, 9, 6, 0, 4, 7, 0,\n",
      "        2, 4, 6, 0, 9, 8, 7, 4])\n",
      "torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [3584 x 28], m2: [784 x 10] at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/TH/generic/THTensorMath.cpp:940",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b8dfc6e129e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mouputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch3env/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch3env/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch3env/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [3584 x 28], m2: [784 x 10] at /opt/conda/conda-bld/pytorch_1544174967633/work/aten/src/TH/generic/THTensorMath.cpp:940"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dl:\n",
    "    print(labels)\n",
    "    print(images.shape)\n",
    "    ouputs = model(images)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error due to wrong dimension of the image. This needs to be flattened. .reshape() is a tensor method which helps to view the image as flat vector. \n",
    "\n",
    "To do so we extend nn.Module class of pytorch and define a custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self): # Constructor\n",
    "        super().__init__() # To call constructor of nn.Module\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 784)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "        \n",
    "model = MnistModel() # Create object of class.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First function is a contructor. __init__ constructor method, we instantiate the weights and biases using nn.Linear.  .reshape() is used to change the view of pytorch tensor. -1 represents that pytorch will decide itself what value to pass based on previous tensor. \n",
    "This model does not have weight and biases, but instead returns list of weight and biases using .parameters(), which is further used in optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784]) torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.0228,  0.0308,  0.0191,  ..., -0.0234, -0.0272,  0.0334],\n",
       "         [-0.0349, -0.0151,  0.0001,  ...,  0.0096, -0.0239,  0.0175],\n",
       "         [-0.0318, -0.0178, -0.0316,  ...,  0.0105,  0.0089,  0.0216],\n",
       "         ...,\n",
       "         [ 0.0163, -0.0243, -0.0282,  ...,  0.0072, -0.0148,  0.0214],\n",
       "         [-0.0334,  0.0171, -0.0338,  ..., -0.0094, -0.0121,  0.0027],\n",
       "         [ 0.0091, -0.0243, -0.0182,  ...,  0.0060,  0.0310,  0.0192]],\n",
       "        requires_grad=True), Parameter containing:\n",
       " tensor([ 0.0331, -0.0143, -0.0217,  0.0200, -0.0320, -0.0330, -0.0072, -0.0233,\n",
       "          0.0126,  0.0296], requires_grad=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model.linear.weight.shape, model.linear.bias.shape)\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:  torch.Size([128, 1, 28, 28])\n",
      "output.shape:  torch.Size([128, 10])\n",
      "Sample outputs:  tensor([[ 0.0227,  0.0833,  0.1454, -0.2663,  0.1954,  0.0615, -0.0631,  0.2042,\n",
      "          0.2042, -0.0046],\n",
      "        [-0.1943,  0.1443,  0.1587,  0.0036, -0.2020, -0.0350,  0.1801, -0.1106,\n",
      "          0.4974, -0.1485]])\n"
     ]
    }
   ],
   "source": [
    "# Using this model  in for-in loop.\n",
    "for images, labels in train_dl:\n",
    "    print('Images: ', images.shape)\n",
    "    output = model(images)\n",
    "    break\n",
    "print('output.shape: ', output.shape)\n",
    "print('Sample outputs: ', output[:2].data) # Gives output for two images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent outputs as probability that should lie between 0 and 1. But here we have negative outputs. so in order to do so we use Softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0227,  0.0833,  0.1454, -0.2663,  0.1954,  0.0615, -0.0631,  0.2042,\n",
       "         0.2042, -0.0046], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0229, 1.0869, 1.1565, 0.7662, 1.2158, 1.0635, 0.9389, 1.2265, 1.2265,\n",
       "        0.9954], grad_fn=<ExpBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take exponential to convert negative to positive values for probability\n",
    "exps = torch.exp(output[0])\n",
    "exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0956, 0.1016, 0.1081, 0.0716, 0.1136, 0.0994, 0.0878, 0.1146, 0.1146,\n",
       "        0.0930], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting numbers that are greater than 1 to lie between 0 and 1\n",
    "prob = exps / torch.sum(exps)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# verifying probability by adding all probabilities which should be equal to 1.\n",
    "print(torch.sum(prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Using built-in torch functions for calclating softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Probabilities:  tensor([[0.0956, 0.1016, 0.1081, 0.0716, 0.1136, 0.0994, 0.0878, 0.1146, 0.1146,\n",
      "         0.0930],\n",
      "        [0.0781, 0.1096, 0.1112, 0.0953, 0.0776, 0.0916, 0.1136, 0.0850, 0.1561,\n",
      "         0.0818]])\n",
      "Sum of probabilities:  1.0\n"
     ]
    }
   ],
   "source": [
    "prob = F.softmax(output, dim=1) # Dimension is 1 because because 0th dimension is for batches and we want to apply softmax on a prticular image\n",
    "\n",
    "print('Sample Probabilities: ', prob[:2].data)\n",
    "\n",
    "# Checking sum of probabilities\n",
    "print('Sum of probabilities: ', torch.sum(prob[0]).item()) # .item() converts zero dimension tensor to a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1146, 0.1561, 0.1555, 0.1329, 0.1306, 0.1193, 0.1360, 0.1313, 0.1256,\n",
      "        0.1397, 0.1309, 0.1479, 0.1320, 0.1515, 0.1425, 0.1446, 0.1421, 0.1253,\n",
      "        0.1317, 0.1349, 0.1365, 0.1232, 0.1236, 0.1164, 0.1436, 0.1293, 0.1202,\n",
      "        0.1320, 0.1344, 0.1281, 0.1278, 0.1351, 0.1225, 0.1372, 0.1227, 0.1545,\n",
      "        0.1157, 0.1203, 0.1370, 0.1488, 0.1733, 0.1166, 0.1385, 0.1117, 0.1247,\n",
      "        0.1197, 0.1204, 0.1352, 0.1357, 0.1292, 0.1293, 0.1230, 0.1290, 0.1893,\n",
      "        0.1379, 0.1243, 0.1216, 0.1423, 0.1220, 0.1165, 0.1256, 0.1275, 0.1188,\n",
      "        0.1444, 0.1240, 0.1325, 0.1349, 0.1245, 0.1160, 0.1398, 0.1213, 0.1250,\n",
      "        0.1248, 0.1254, 0.1414, 0.1370, 0.1208, 0.1176, 0.1500, 0.1274, 0.1330,\n",
      "        0.1298, 0.1167, 0.1313, 0.1237, 0.1270, 0.1182, 0.1397, 0.1103, 0.1336,\n",
      "        0.1255, 0.1200, 0.1347, 0.1174, 0.1436, 0.1195, 0.1439, 0.1298, 0.1259,\n",
      "        0.1368, 0.1388, 0.1399, 0.1440, 0.1240, 0.1158, 0.1376, 0.1163, 0.1283,\n",
      "        0.1237, 0.1393, 0.1209, 0.1528, 0.1218, 0.1286, 0.1248, 0.1293, 0.1256,\n",
      "        0.1412, 0.1174, 0.1341, 0.1189, 0.1279, 0.1230, 0.1605, 0.1385, 0.1237,\n",
      "        0.1188, 0.1344], grad_fn=<MaxBackward0>)\n",
      "tensor([8, 8, 0, 9, 8, 1, 0, 8, 2, 0, 0, 0, 6, 8, 0, 6, 0, 8, 6, 8, 6, 2, 0, 0,\n",
      "        0, 0, 6, 8, 0, 6, 0, 8, 0, 6, 6, 6, 1, 4, 0, 0, 8, 9, 8, 9, 0, 1, 6, 0,\n",
      "        6, 2, 4, 0, 6, 8, 8, 8, 6, 8, 7, 0, 2, 0, 6, 6, 8, 4, 0, 1, 8, 0, 0, 0,\n",
      "        6, 9, 0, 8, 8, 1, 8, 0, 8, 8, 2, 9, 0, 2, 0, 8, 1, 0, 0, 2, 8, 6, 8, 2,\n",
      "        2, 8, 0, 2, 0, 8, 0, 8, 8, 0, 1, 9, 0, 2, 0, 0, 6, 0, 0, 8, 5, 6, 0, 8,\n",
      "        0, 8, 9, 9, 8, 0, 0, 4])\n"
     ]
    }
   ],
   "source": [
    "# Predict label by choosing index with highest probability.\n",
    "max_prob, predict = torch.max(prob, dim=1)\n",
    "print(max_prob)\n",
    "print(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7, 2, 3, 5, 7, 9, 5, 8, 8, 9, 9, 9, 5, 2, 0, 4, 3, 8, 7, 5, 8, 1, 1,\n",
       "        8, 3, 7, 9, 2, 7, 1, 5, 3, 2, 0, 2, 7, 4, 9, 1, 0, 3, 7, 1, 7, 1, 3, 2,\n",
       "        0, 5, 2, 9, 2, 0, 8, 7, 4, 8, 8, 8, 2, 7, 3, 3, 5, 6, 9, 5, 9, 7, 2, 1,\n",
       "        9, 5, 4, 0, 7, 7, 0, 7, 5, 0, 7, 1, 6, 7, 6, 5, 9, 1, 1, 8, 5, 7, 5, 4,\n",
       "        6, 3, 7, 2, 8, 5, 7, 8, 6, 9, 9, 1, 8, 0, 6, 1, 3, 4, 4, 5, 5, 8, 9, 3,\n",
       "        1, 7, 6, 2, 3, 8, 5, 5])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compaing predicted with labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evauate the model by calculating the accuracy. We compare all the predictions with labels and divide it total number of predictions. While comparing we get 0 for all the predictions not equal to labels and 1s for all equal ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    _, predict = torch.max(output, dim=1)\n",
    "    return torch.tensor(torch.sum(predict == labels).item()/ len(predict) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0625)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(output, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not suffient for calculating the loss because it is not differentiable. Therefore we use cross-entropy to calculate loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.cross_entropy # cross_entropy has built-in softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3338, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(output, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is identical to that done in linear regression except an additional validation phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, x): # funtion to flatten image\n",
    "        x = x.reshape(-1, 784)\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    def training_set(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images) # Generate Predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch(self, output):\n",
    "        batch_losses = [x['val_loss'] for x in output]\n",
    "        epoch_losses = torch.stack(batch_losses).mean()\n",
    "        batch_accuracies = [x['val_acc'] for x in output]\n",
    "        epoch_accuracies = torch.stack(batch_accuracies).mean()\n",
    "        return {'val_loss':epoch_losses.item(), 'val_acc': epoch_accuracies.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "         print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))        \n",
    "\n",
    "model = MnistModel()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dl):\n",
    "    output = [model.validation_step(batch) for batch in val_dl]\n",
    "    return model.validation_epoch(output)\n",
    "\n",
    "# Directly check evaluation function\n",
    "#result0 = evaluate(model, val_dl)\n",
    "#result0\n",
    "def fit(epochs, lr, model, train_dl, val_dl, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_dl:\n",
    "            loss = model.training_set(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_dl)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'val_loss': 2.327930212020874, 'val_acc': 0.09711234271526337}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result0 = evaluate(model, val_dl)\n",
    "result0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.9545, val_acc: 0.6488\n",
      "Epoch [1], val_loss: 1.6854, val_acc: 0.7330\n",
      "Epoch [2], val_loss: 1.4842, val_acc: 0.7658\n",
      "Epoch [3], val_loss: 1.3322, val_acc: 0.7839\n",
      "Epoch [4], val_loss: 1.2155, val_acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "history1 = fit(5, 0.001, model, train_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 1.1240, val_acc: 0.8024\n",
      "Epoch [1], val_loss: 1.0506, val_acc: 0.8087\n",
      "Epoch [2], val_loss: 0.9906, val_acc: 0.8143\n",
      "Epoch [3], val_loss: 0.9406, val_acc: 0.8202\n",
      "Epoch [4], val_loss: 0.8985, val_acc: 0.8236\n"
     ]
    }
   ],
   "source": [
    "history2 = fit(5, 0.001, model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.8623, val_acc: 0.8285\n",
      "Epoch [1], val_loss: 0.8310, val_acc: 0.8323\n",
      "Epoch [2], val_loss: 0.8036, val_acc: 0.8333\n",
      "Epoch [3], val_loss: 0.7794, val_acc: 0.8353\n",
      "Epoch [4], val_loss: 0.7578, val_acc: 0.8376\n"
     ]
    }
   ],
   "source": [
    "history3 = fit(5, 0.001, model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], val_loss: 0.7027, val_acc: 0.8521\n",
      "Epoch [1], val_loss: 0.6866, val_acc: 0.8534\n",
      "Epoch [2], val_loss: 0.6718, val_acc: 0.8551\n",
      "Epoch [3], val_loss: 0.6583, val_acc: 0.8564\n",
      "Epoch [4], val_loss: 0.6459, val_acc: 0.8578\n"
     ]
    }
   ],
   "source": [
    "history4 = fit(5, 0.001, model, train_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace all values with result\n",
    "history = [result0] + history1 + history2 + history3 + history4\n",
    "accuracies = [result['val_acc'] for result in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
